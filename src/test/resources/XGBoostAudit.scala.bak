import java.nio.file.{Files, Paths}
import java.io._
import java.io.{BufferedReader, FileReader}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{Binarizer, Imputer, StringIndexer, VectorAssembler, VectorIndexer}
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.sql.functions.{lit, udf}
import org.apache.spark.sql.types.{BooleanType, DataType, DoubleType, IntegerType, StringType, StructField, StructType}
import ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier

var df = spark.read.option("header", "true").option("inferSchema", "true").csv("csv/Audit.csv")
val schema = {
  val is = new FileReader("schema/Audit.json")
  val br = new BufferedReader(is)
  val schema = DataType.fromJson(br.readLine())
  schema.asInstanceOf[StructType]
}
val fields = schema.fields
df = {
  for(field <- fields){
    val column = df.apply(field.name).cast(field.dataType);
    df = df.withColumn("tmp_" + field.name, column).drop(field.name).withColumnRenamed("tmp_" + field.name, field.name)
  }
  df
}
val stringIndexer = fields.filter(field => field.dataType.isInstanceOf[StringType]).map(field => {
    new StringIndexer().setInputCol(field.name).setOutputCol(field.name + "Index")
})
val stringIndexerColName = fields.filter(field => field.dataType.isInstanceOf[StringType]).map(field => field.name+"Index")
val numberColName = fields.filter(field => {
  (field.dataType.isInstanceOf[DoubleType] || field.dataType.isInstanceOf[IntegerType] || field.dataType.isInstanceOf[BooleanType]) && !field.name.equals("Adjusted")
}).map(field => field.name)
val assembler = new VectorAssembler().setInputCols(stringIndexerColName ++ numberColName).setOutputCol("features")
val classifier = new XGBoostClassifier(Map("objective" -> "binary:logistic", "num_round" -> 10, "missing" -> 0.0, "allow_non_zero_missing" -> "true")).setLabelCol("Adjusted").setFeaturesCol("features")
val pipeline = new Pipeline().setStages(Array(new Pipeline().setStages(stringIndexer ++ Array(assembler)), classifier))
val pipelineModel = pipeline.fit(df)
pipelineModel.write.overwrite.save("pipeline/XGBoostAudit")

var xgbDf = pipelineModel.transform(df)
val vectorToColumn = udf{ (vec: Vector, index: Int) => vec(index).toFloat }
xgbDf = xgbDf.selectExpr("prediction as Adjusted", "probability")
xgbDf = xgbDf.withColumn("AdjustedTmp", xgbDf("Adjusted").cast(IntegerType).cast(StringType)).drop("Adjusted").withColumnRenamed("AdjustedTmp", "Adjusted")
xgbDf = xgbDf.withColumn("probability(0)", vectorToColumn(xgbDf("probability"), lit(0))).withColumn("probability(1)", vectorToColumn(xgbDf("probability"), lit(1))).drop("probability")
xgbDf.coalesce(1).write.mode("overwrite").format("com.databricks.spark.csv").option("header", "true").save("csv/XGBoostAudit.csv")

import org.jpmml.sparkml.PMMLBuilder
import org.jpmml.sparkml.model.HasRegressionTableOptions
var precision = 1e-14
var zeroThreshold = 1e-14
val pmmlBuilder = new PMMLBuilder(schema, pipelineModel).verify(df, precision, zeroThreshold)
val pmml = pmmlBuilder.build
